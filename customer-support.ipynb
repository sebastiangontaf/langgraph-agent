{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6da721a-f83d-4c14-ac97-517d3ac8ea6f",
   "metadata": {},
   "source": [
    "# Build a Customer Support Bot\n",
    "\n",
    "Customer support bots can free up teams' time by handling routine issues, but it can be hard to build a bot that reliably handles diverse tasks in a way that doesn't leave the user pulling their hair out.\n",
    "\n",
    "In this tutorial, you will build a customer support bot for an airline to help users research and make travel arrangements. You'll learn to use LangGraph's interrupts and checkpointers and more complex state to organize your assistant's tools and manage a user's flight bookings, hotel reservations, car rentals, and excursions. It assumes you are familiar with the concepts presented in the [LangGraph introductory tutorial](https://langchain-ai.github.io/langgraph/tutorials/introduction/).\n",
    "\n",
    "By the end, you'll have built a working bot and gained an understanding of  LangGraph's key concepts and architectures. You'll be able to apply these design patterns to your other AI projects.\n",
    "\n",
    "Your final chat bot will look something like the following diagram:\n",
    "\n",
    "![Final Diagram](../img/part-4-diagram.png)\n",
    "\n",
    "Let's start!\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "First, set up your environment. We'll install this tutorial's prerequisites, download the test DB, and define the tools we will reuse in each section.\n",
    "\n",
    "We'll be using Claude as our LLM and define a number of custom tools. While most of our tools will connect to a local sqlite database (and require no additional dependencies), we will also provide a general web search to the agent using Tavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc570bf-e129-415b-8f2d-8bbce08131ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "% pip install -U langgraph langchain-community langchain-anthropic tavily-python pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "358e5666-b7c5-4e46-90a1-7ea273d86ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"ANTHROPIC_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "\n",
    "# Recommended\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Customer Support Bot Tutorial\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58121817-b31e-496d-9e46-2bec02c63300",
   "metadata": {},
   "source": [
    "#### Populate the database\n",
    "\n",
    "Run the next script to fetch a `sqlite` DB we've prepared for this tutorial and update it to look like it's current. The details are unimportant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71638c2a-5038-439e-907a-de2bb548db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sqlite3\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "db_url = \"https://storage.googleapis.com/benchmarks-artifacts/travel-db/travel2.sqlite\"\n",
    "local_file = \"travel2.sqlite\"\n",
    "# The backup lets us restart for each tutorial section\n",
    "backup_file = \"travel2.backup.sqlite\"\n",
    "overwrite = False\n",
    "if overwrite or not os.path.exists(local_file):\n",
    "    response = requests.get(db_url)\n",
    "    response.raise_for_status()  # Ensure the request was successful\n",
    "    with open(local_file, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    # Backup - we will use this to \"reset\" our DB in each section\n",
    "    shutil.copy(local_file, backup_file)\n",
    "# Convert the flights to present time for our tutorial\n",
    "conn = sqlite3.connect(local_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "tables = pd.read_sql(\n",
    "    \"SELECT name FROM sqlite_master WHERE type='table';\", conn\n",
    ").name.tolist()\n",
    "tdf = {}\n",
    "for t in tables:\n",
    "    tdf[t] = pd.read_sql(f\"SELECT * from {t}\", conn)\n",
    "\n",
    "example_time = pd.to_datetime(\n",
    "    tdf[\"flights\"][\"actual_departure\"].replace(\"\\\\N\", pd.NaT)\n",
    ").max()\n",
    "current_time = pd.to_datetime(\"now\").tz_localize(example_time.tz)\n",
    "time_diff = current_time - example_time\n",
    "\n",
    "tdf[\"bookings\"][\"book_date\"] = (\n",
    "    pd.to_datetime(tdf[\"bookings\"][\"book_date\"].replace(\"\\\\N\", pd.NaT), utc=True)\n",
    "    + time_diff\n",
    ")\n",
    "\n",
    "datetime_columns = [\n",
    "    \"scheduled_departure\",\n",
    "    \"scheduled_arrival\",\n",
    "    \"actual_departure\",\n",
    "    \"actual_arrival\",\n",
    "]\n",
    "for column in datetime_columns:\n",
    "    tdf[\"flights\"][column] = (\n",
    "        pd.to_datetime(tdf[\"flights\"][column].replace(\"\\\\N\", pd.NaT)) + time_diff\n",
    "    )\n",
    "\n",
    "for table_name, df in tdf.items():\n",
    "    df.to_sql(table_name, conn, if_exists=\"replace\", index=False)\n",
    "del df\n",
    "del tdf\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "db = local_file  # We'll be using this local file as our DB in this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3aa34e-923b-49a1-8f34-54a1b2a90825",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "Next, define our assistant's tools to search the airline's policy manual and search and manage reservations for flights, hotels, car rentals, and excursions. We will reuse these tools throughout the tutorial. The exact implementations\n",
    "aren't important, so feel free to run the code below and jump to [Part 1](#part-1-zero-shot).\n",
    "\n",
    "#### Lookup Company Policies\n",
    "\n",
    "The assistant retrieve policy information to answer user questions. Note that _enforcement_ of these policies still must be done within the tools/APIs themselves, since the LLM can always ignore this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "654e2f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import openai\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "response = requests.get(\n",
    "    \"https://storage.googleapis.com/benchmarks-artifacts/travel-db/swiss_faq.md\"\n",
    ")\n",
    "response.raise_for_status()\n",
    "faq_text = response.text\n",
    "\n",
    "docs = [{\"page_content\": txt} for txt in re.split(r\"(?=\\n##)\", faq_text)]\n",
    "\n",
    "\n",
    "class VectorStoreRetriever:\n",
    "    def __init__(self, docs: list, vectors: list, oai_client):\n",
    "        self._arr = np.array(vectors)\n",
    "        self._docs = docs\n",
    "        self._client = oai_client\n",
    "\n",
    "    @classmethod\n",
    "    def from_docs(cls, docs, oai_client):\n",
    "        embeddings = oai_client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\", input=[doc[\"page_content\"] for doc in docs]\n",
    "        )\n",
    "        vectors = [emb.embedding for emb in embeddings.data]\n",
    "        return cls(docs, vectors, oai_client)\n",
    "\n",
    "    def query(self, query: str, k: int = 5) -> list[dict]:\n",
    "        embed = self._client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\", input=[query]\n",
    "        )\n",
    "        # \"@\" is just a matrix multiplication in python\n",
    "        scores = np.array(embed.data[0].embedding) @ self._arr.T\n",
    "        top_k_idx = np.argpartition(scores, -k)[-k:]\n",
    "        top_k_idx_sorted = top_k_idx[np.argsort(-scores[top_k_idx])]\n",
    "        return [\n",
    "            {**self._docs[idx], \"similarity\": scores[idx]} for idx in top_k_idx_sorted\n",
    "        ]\n",
    "\n",
    "\n",
    "retriever = VectorStoreRetriever.from_docs(docs, openai.Client())\n",
    "\n",
    "\n",
    "@tool\n",
    "def lookup_policy(query: str) -> str:\n",
    "    \"\"\"Consult the company policies to check whether certain options are permitted.\n",
    "    Use this before making any flight changes performing other 'write' events.\"\"\"\n",
    "    docs = retriever.query(query, k=2)\n",
    "    return \"\\n\\n\".join([doc[\"page_content\"] for doc in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3556949",
   "metadata": {},
   "source": [
    "#### Flights\n",
    "\n",
    "Define the (`fetch_user_flight_information`) tool to let the agent see the current user's flight information.  Then define tools to search for flights and manage the passenger's bookings stored in the SQL database.\n",
    "\n",
    "We use `ensure_config` to pass in the `passenger_id` in via configurable parameters. The LLM never has to provide these explicitly, they are provided for a given invocation of the graph so that each user cannot access other passengers' booking information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "043b4341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from datetime import date, datetime\n",
    "from typing import Optional\n",
    "\n",
    "import pytz\n",
    "from langchain_core.runnables import ensure_config\n",
    "\n",
    "\n",
    "@tool\n",
    "def fetch_user_flight_information() -> list[dict]:\n",
    "    \"\"\"Fetch all tickets for the user along with corresponding flight information and seat assignments.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries where each dictionary contains the ticket details,\n",
    "        associated flight details, and the seat assignments for each ticket belonging to the user.\n",
    "    \"\"\"\n",
    "    config = ensure_config()  # Fetch from the context\n",
    "    configuration = config.get(\"configurable\", {})\n",
    "    passenger_id = configuration.get(\"passenger_id\", None)\n",
    "    if not passenger_id:\n",
    "        raise ValueError(\"No passenger ID configured.\")\n",
    "\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        t.ticket_no, t.book_ref,\n",
    "        f.flight_id, f.flight_no, f.departure_airport, f.arrival_airport, f.scheduled_departure, f.scheduled_arrival,\n",
    "        bp.seat_no, tf.fare_conditions\n",
    "    FROM \n",
    "        tickets t\n",
    "        JOIN ticket_flights tf ON t.ticket_no = tf.ticket_no\n",
    "        JOIN flights f ON tf.flight_id = f.flight_id\n",
    "        JOIN boarding_passes bp ON bp.ticket_no = t.ticket_no AND bp.flight_id = f.flight_id\n",
    "    WHERE \n",
    "        t.passenger_id = ?\n",
    "    \"\"\"\n",
    "    cursor.execute(query, (passenger_id,))\n",
    "    rows = cursor.fetchall()\n",
    "    column_names = [column[0] for column in cursor.description]\n",
    "    results = [dict(zip(column_names, row)) for row in rows]\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_flights(\n",
    "    departure_airport: Optional[str] = None,\n",
    "    arrival_airport: Optional[str] = None,\n",
    "    start_time: Optional[date | datetime] = None,\n",
    "    end_time: Optional[date | datetime] = None,\n",
    "    limit: int = 20,\n",
    ") -> list[dict]:\n",
    "    \"\"\"Search for flights based on departure airport, arrival airport, and departure time range.\"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    query = \"SELECT * FROM flights WHERE 1 = 1\"\n",
    "    params = []\n",
    "\n",
    "    if departure_airport:\n",
    "        query += \" AND departure_airport = ?\"\n",
    "        params.append(departure_airport)\n",
    "\n",
    "    if arrival_airport:\n",
    "        query += \" AND arrival_airport = ?\"\n",
    "        params.append(arrival_airport)\n",
    "\n",
    "    if start_time:\n",
    "        query += \" AND scheduled_departure >= ?\"\n",
    "        params.append(start_time)\n",
    "\n",
    "    if end_time:\n",
    "        query += \" AND scheduled_departure <= ?\"\n",
    "        params.append(end_time)\n",
    "    query += \" LIMIT ?\"\n",
    "    params.append(limit)\n",
    "    cursor.execute(query, params)\n",
    "    rows = cursor.fetchall()\n",
    "    column_names = [column[0] for column in cursor.description]\n",
    "    results = [dict(zip(column_names, row)) for row in rows]\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "@tool\n",
    "def update_ticket_to_new_flight(ticket_no: str, new_flight_id: int) -> str:\n",
    "    \"\"\"Update the user's ticket to a new valid flight.\"\"\"\n",
    "    config = ensure_config()\n",
    "    configuration = config.get(\"configurable\", {})\n",
    "    passenger_id = configuration.get(\"passenger_id\", None)\n",
    "    if not passenger_id:\n",
    "        raise ValueError(\"No passenger ID configured.\")\n",
    "\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\n",
    "        \"SELECT departure_airport, arrival_airport, scheduled_departure FROM flights WHERE flight_id = ?\",\n",
    "        (new_flight_id,),\n",
    "    )\n",
    "    new_flight = cursor.fetchone()\n",
    "    if not new_flight:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return \"Invalid new flight ID provided.\"\n",
    "    column_names = [column[0] for column in cursor.description]\n",
    "    new_flight_dict = dict(zip(column_names, new_flight))\n",
    "    timezone = pytz.timezone(\"Etc/GMT-3\")\n",
    "    current_time = datetime.now(tz=timezone)\n",
    "    departure_time = datetime.strptime(\n",
    "        new_flight_dict[\"scheduled_departure\"], \"%Y-%m-%d %H:%M:%S.%f%z\"\n",
    "    )\n",
    "    time_until = (departure_time - current_time).total_seconds()\n",
    "    if time_until < (3 * 3600):\n",
    "        return f\"Not permitted to reschedule to a flight that is less than 3 hours from the current time. Selected flight is at {departure_time}.\"\n",
    "\n",
    "    cursor.execute(\n",
    "        \"SELECT flight_id FROM ticket_flights WHERE ticket_no = ?\", (ticket_no,)\n",
    "    )\n",
    "    current_flight = cursor.fetchone()\n",
    "    if not current_flight:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return \"No existing ticket found for the given ticket number.\"\n",
    "\n",
    "    # Check the signed-in user actually has this ticket\n",
    "    cursor.execute(\n",
    "        \"SELECT * FROM tickets WHERE ticket_no = ? AND passenger_id = ?\",\n",
    "        (ticket_no, passenger_id),\n",
    "    )\n",
    "    current_ticket = cursor.fetchone()\n",
    "    if not current_ticket:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return f\"Current signed-in passenger with ID {passenger_id} not the owner of ticket {ticket_no}\"\n",
    "\n",
    "    # In a real application, you'd likely add additional checks here to enforce business logic,\n",
    "    # like \"does the new departure airport match the current ticket\", etc.\n",
    "    # While it's best to try to be *proactive* in 'type-hinting' policies to the LLM\n",
    "    # it's inevitably going to get things wrong, so you **also** need to ensure your\n",
    "    # API enforces valid behavior\n",
    "    cursor.execute(\n",
    "        \"UPDATE ticket_flights SET flight_id = ? WHERE ticket_no = ?\",\n",
    "        (new_flight_id, ticket_no),\n",
    "    )\n",
    "    conn.commit()\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    return \"Ticket successfully updated to new flight.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def cancel_ticket(ticket_no: str) -> str:\n",
    "    \"\"\"Cancel the user's ticket and remove it from the database.\"\"\"\n",
    "    config = ensure_config()\n",
    "    configuration = config.get(\"configurable\", {})\n",
    "    passenger_id = configuration.get(\"passenger_id\", None)\n",
    "    if not passenger_id:\n",
    "        raise ValueError(\"No passenger ID configured.\")\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\n",
    "        \"SELECT flight_id FROM ticket_flights WHERE ticket_no = ?\", (ticket_no,)\n",
    "    )\n",
    "    existing_ticket = cursor.fetchone()\n",
    "    if not existing_ticket:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return \"No existing ticket found for the given ticket number.\"\n",
    "\n",
    "    # Check the signed-in user actually has this ticket\n",
    "    cursor.execute(\n",
    "        \"SELECT flight_id FROM tickets WHERE ticket_no = ? AND passenger_id = ?\",\n",
    "        (ticket_no, passenger_id),\n",
    "    )\n",
    "    current_ticket = cursor.fetchone()\n",
    "    if not current_ticket:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return f\"Current signed-in passenger with ID {passenger_id} not the owner of ticket {ticket_no}\"\n",
    "\n",
    "    cursor.execute(\"DELETE FROM ticket_flights WHERE ticket_no = ?\", (ticket_no,))\n",
    "    conn.commit()\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    return \"Ticket successfully cancelled.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf77f8f-a051-46cd-be0b-7fe69121a3c1",
   "metadata": {},
   "source": [
    "#### Car Rental Tools\n",
    "\n",
    "Once a user books a flight, they likely will want to organize transportation. Define some \"car rental\" tools to let the user search for and reserve a car at their destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3edabaf-7a23-4f9f-9c57-97b799bc21df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime\n",
    "from typing import Optional, Union\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_car_rentals(\n",
    "    location: Optional[str] = None,\n",
    "    name: Optional[str] = None,\n",
    "    price_tier: Optional[str] = None,\n",
    "    start_date: Optional[Union[datetime, date]] = None,\n",
    "    end_date: Optional[Union[datetime, date]] = None,\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Search for car rentals based on location, name, price tier, start date, and end date.\n",
    "\n",
    "    Args:\n",
    "        location (Optional[str]): The location of the car rental. Defaults to None.\n",
    "        name (Optional[str]): The name of the car rental company. Defaults to None.\n",
    "        price_tier (Optional[str]): The price tier of the car rental. Defaults to None.\n",
    "        start_date (Optional[Union[datetime, date]]): The start date of the car rental. Defaults to None.\n",
    "        end_date (Optional[Union[datetime, date]]): The end date of the car rental. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of car rental dictionaries matching the search criteria.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    query = \"SELECT * FROM car_rentals WHERE 1=1\"\n",
    "    params = []\n",
    "\n",
    "    if location:\n",
    "        query += \" AND location LIKE ?\"\n",
    "        params.append(f\"%{location}%\")\n",
    "    if name:\n",
    "        query += \" AND name LIKE ?\"\n",
    "        params.append(f\"%{name}%\")\n",
    "    # For our tutorial, we will let you match on any dates and price tier.\n",
    "    # (since our toy dataset doesn't have much data)\n",
    "    cursor.execute(query, params)\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    return [\n",
    "        dict(zip([column[0] for column in cursor.description], row)) for row in results\n",
    "    ]\n",
    "\n",
    "\n",
    "@tool\n",
    "def book_car_rental(rental_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Book a car rental by its ID.\n",
    "\n",
    "    Args:\n",
    "        rental_id (int): The ID of the car rental to book.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating whether the car rental was successfully booked or not.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"UPDATE car_rentals SET booked = 1 WHERE id = ?\", (rental_id,))\n",
    "    conn.commit()\n",
    "\n",
    "    if cursor.rowcount > 0:\n",
    "        conn.close()\n",
    "        return f\"Car rental {rental_id} successfully booked.\"\n",
    "    else:\n",
    "        conn.close()\n",
    "        return f\"No car rental found with ID {rental_id}.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def update_car_rental(\n",
    "    rental_id: int,\n",
    "    start_date: Optional[Union[datetime, date]] = None,\n",
    "    end_date: Optional[Union[datetime, date]] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Update a car rental's start and end dates by its ID.\n",
    "\n",
    "    Args:\n",
    "        rental_id (int): The ID of the car rental to update.\n",
    "        start_date (Optional[Union[datetime, date]]): The new start date of the car rental. Defaults to None.\n",
    "        end_date (Optional[Union[datetime, date]]): The new end date of the car rental. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating whether the car rental was successfully updated or not.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    if start_date:\n",
    "        cursor.execute(\n",
    "            \"UPDATE car_rentals SET start_date = ? WHERE id = ?\",\n",
    "            (start_date, rental_id),\n",
    "        )\n",
    "    if end_date:\n",
    "        cursor.execute(\n",
    "            \"UPDATE car_rentals SET end_date = ? WHERE id = ?\", (end_date, rental_id)\n",
    "        )\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    if cursor.rowcount > 0:\n",
    "        conn.close()\n",
    "        return f\"Car rental {rental_id} successfully updated.\"\n",
    "    else:\n",
    "        conn.close()\n",
    "        return f\"No car rental found with ID {rental_id}.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def cancel_car_rental(rental_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Cancel a car rental by its ID.\n",
    "\n",
    "    Args:\n",
    "        rental_id (int): The ID of the car rental to cancel.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating whether the car rental was successfully cancelled or not.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"UPDATE car_rentals SET booked = 0 WHERE id = ?\", (rental_id,))\n",
    "    conn.commit()\n",
    "\n",
    "    if cursor.rowcount > 0:\n",
    "        conn.close()\n",
    "        return f\"Car rental {rental_id} successfully cancelled.\"\n",
    "    else:\n",
    "        conn.close()\n",
    "        return f\"No car rental found with ID {rental_id}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c5712-d2b1-492a-a7b7-4396aa4ec339",
   "metadata": {},
   "source": [
    "#### Hotels\n",
    "\n",
    "The user has to sleep! Define some tools to search for and manage hotel reservations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8e4ab3c-0086-4257-855b-97cc4037513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search_hotels(\n",
    "    location: Optional[str] = None,\n",
    "    name: Optional[str] = None,\n",
    "    price_tier: Optional[str] = None,\n",
    "    checkin_date: Optional[Union[datetime, date]] = None,\n",
    "    checkout_date: Optional[Union[datetime, date]] = None,\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Search for hotels based on location, name, price tier, check-in date, and check-out date.\n",
    "\n",
    "    Args:\n",
    "        location (Optional[str]): The location of the hotel. Defaults to None.\n",
    "        name (Optional[str]): The name of the hotel. Defaults to None.\n",
    "        price_tier (Optional[str]): The price tier of the hotel. Defaults to None. Examples: Midscale, Upper Midscale, Upscale, Luxury\n",
    "        checkin_date (Optional[Union[datetime, date]]): The check-in date of the hotel. Defaults to None.\n",
    "        checkout_date (Optional[Union[datetime, date]]): The check-out date of the hotel. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of hotel dictionaries matching the search criteria.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    query = \"SELECT * FROM hotels WHERE 1=1\"\n",
    "    params = []\n",
    "\n",
    "    if location:\n",
    "        query += \" AND location LIKE ?\"\n",
    "        params.append(f\"%{location}%\")\n",
    "    if name:\n",
    "        query += \" AND name LIKE ?\"\n",
    "        params.append(f\"%{name}%\")\n",
    "    # For the sake of this tutorial, we will let you match on any dates and price tier.\n",
    "    cursor.execute(query, params)\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    return [\n",
    "        dict(zip([column[0] for column in cursor.description], row)) for row in results\n",
    "    ]\n",
    "\n",
    "\n",
    "@tool\n",
    "def book_hotel(hotel_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Book a hotel by its ID.\n",
    "\n",
    "    Args:\n",
    "        hotel_id (int): The ID of the hotel to book.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating whether the hotel was successfully booked or not.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"UPDATE hotels SET booked = 1 WHERE id = ?\", (hotel_id,))\n",
    "    conn.commit()\n",
    "\n",
    "    if cursor.rowcount > 0:\n",
    "        conn.close()\n",
    "        return f\"Hotel {hotel_id} successfully booked.\"\n",
    "    else:\n",
    "        conn.close()\n",
    "        return f\"No hotel found with ID {hotel_id}.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def update_hotel(\n",
    "    hotel_id: int,\n",
    "    checkin_date: Optional[Union[datetime, date]] = None,\n",
    "    checkout_date: Optional[Union[datetime, date]] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Update a hotel's check-in and check-out dates by its ID.\n",
    "\n",
    "    Args:\n",
    "        hotel_id (int): The ID of the hotel to update.\n",
    "        checkin_date (Optional[Union[datetime, date]]): The new check-in date of the hotel. Defaults to None.\n",
    "        checkout_date (Optional[Union[datetime, date]]): The new check-out date of the hotel. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating whether the hotel was successfully updated or not.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    if checkin_date:\n",
    "        cursor.execute(\n",
    "            \"UPDATE hotels SET checkin_date = ? WHERE id = ?\", (checkin_date, hotel_id)\n",
    "        )\n",
    "    if checkout_date:\n",
    "        cursor.execute(\n",
    "            \"UPDATE hotels SET checkout_date = ? WHERE id = ?\",\n",
    "            (checkout_date, hotel_id),\n",
    "        )\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    if cursor.rowcount > 0:\n",
    "        conn.close()\n",
    "        return f\"Hotel {hotel_id} successfully updated.\"\n",
    "    else:\n",
    "        conn.close()\n",
    "        return f\"No hotel found with ID {hotel_id}.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def cancel_hotel(hotel_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Cancel a hotel by its ID.\n",
    "\n",
    "    Args:\n",
    "        hotel_id (int): The ID of the hotel to cancel.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating whether the hotel was successfully cancelled or not.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"UPDATE hotels SET booked = 0 WHERE id = ?\", (hotel_id,))\n",
    "    conn.commit()\n",
    "\n",
    "    if cursor.rowcount > 0:\n",
    "        conn.close()\n",
    "        return f\"Hotel {hotel_id} successfully cancelled.\"\n",
    "    else:\n",
    "        conn.close()\n",
    "        return f\"No hotel found with ID {hotel_id}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f08190c-21f6-4a07-b9e2-3aa991fe4eed",
   "metadata": {},
   "source": [
    "#### Excursions\n",
    "\n",
    "Finally, define some tools to let the user search for things to do (and make reservations) once they arrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2260eccb-8ae2-4a41-a1ba-f78ee3df3010",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search_trip_recommendations(\n",
    "    location: Optional[str] = None,\n",
    "    name: Optional[str] = None,\n",
    "    keywords: Optional[str] = None,\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Search for trip recommendations based on location, name, and keywords.\n",
    "\n",
    "    Args:\n",
    "        location (Optional[str]): The location of the trip recommendation. Defaults to None.\n",
    "        name (Optional[str]): The name of the trip recommendation. Defaults to None.\n",
    "        keywords (Optional[str]): The keywords associated with the trip recommendation. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of trip recommendation dictionaries matching the search criteria.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    query = \"SELECT * FROM trip_recommendations WHERE 1=1\"\n",
    "    params = []\n",
    "\n",
    "    if location:\n",
    "        query += \" AND location LIKE ?\"\n",
    "        params.append(f\"%{location}%\")\n",
    "    if name:\n",
    "        query += \" AND name LIKE ?\"\n",
    "        params.append(f\"%{name}%\")\n",
    "    if keywords:\n",
    "        keyword_list = keywords.split(\",\")\n",
    "        keyword_conditions = \" OR \".join([\"keywords LIKE ?\" for _ in keyword_list])\n",
    "        query += f\" AND ({keyword_conditions})\"\n",
    "        params.extend([f\"%{keyword.strip()}%\" for keyword in keyword_list])\n",
    "\n",
    "    cursor.execute(query, params)\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    return [\n",
    "        dict(zip([column[0] for column in cursor.description], row)) for row in results\n",
    "    ]\n",
    "\n",
    "\n",
    "@tool\n",
    "def book_excursion(recommendation_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Book a excursion by its recommendation ID.\n",
    "\n",
    "    Args:\n",
    "        recommendation_id (int): The ID of the trip recommendation to book.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating whether the trip recommendation was successfully booked or not.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\n",
    "        \"UPDATE trip_recommendations SET booked = 1 WHERE id = ?\", (recommendation_id,)\n",
    "    )\n",
    "    conn.commit()\n",
    "\n",
    "    if cursor.rowcount > 0:\n",
    "        conn.close()\n",
    "        return f\"Trip recommendation {recommendation_id} successfully booked.\"\n",
    "    else:\n",
    "        conn.close()\n",
    "        return f\"No trip recommendation found with ID {recommendation_id}.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def update_excursion(recommendation_id: int, details: str) -> str:\n",
    "    \"\"\"\n",
    "    Update a trip recommendation's details by its ID.\n",
    "\n",
    "    Args:\n",
    "        recommendation_id (int): The ID of the trip recommendation to update.\n",
    "        details (str): The new details of the trip recommendation.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating whether the trip recommendation was successfully updated or not.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\n",
    "        \"UPDATE trip_recommendations SET details = ? WHERE id = ?\",\n",
    "        (details, recommendation_id),\n",
    "    )\n",
    "    conn.commit()\n",
    "\n",
    "    if cursor.rowcount > 0:\n",
    "        conn.close()\n",
    "        return f\"Trip recommendation {recommendation_id} successfully updated.\"\n",
    "    else:\n",
    "        conn.close()\n",
    "        return f\"No trip recommendation found with ID {recommendation_id}.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def cancel_excursion(recommendation_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Cancel a trip recommendation by its ID.\n",
    "\n",
    "    Args:\n",
    "        recommendation_id (int): The ID of the trip recommendation to cancel.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating whether the trip recommendation was successfully cancelled or not.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\n",
    "        \"UPDATE trip_recommendations SET booked = 0 WHERE id = ?\", (recommendation_id,)\n",
    "    )\n",
    "    conn.commit()\n",
    "\n",
    "    if cursor.rowcount > 0:\n",
    "        conn.close()\n",
    "        return f\"Trip recommendation {recommendation_id} successfully cancelled.\"\n",
    "    else:\n",
    "        conn.close()\n",
    "        return f\"No trip recommendation found with ID {recommendation_id}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf5d064",
   "metadata": {},
   "source": [
    "#### Utilities\n",
    "\n",
    "Define helper functions to pretty print the messages in the graph while we debug it and to give our tool node error handling (by adding the error to the chat history)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "663f001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "def handle_tool_error(state) -> dict:\n",
    "    error = state.get(\"error\")\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n",
    "                tool_call_id=tc[\"id\"],\n",
    "            )\n",
    "            for tc in tool_calls\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def create_tool_node_with_fallback(tools: list) -> dict:\n",
    "    return ToolNode(tools).with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _print_event(event: dict, _printed: set, max_length=1500):\n",
    "    current_state = event.get(\"dialog_state\")\n",
    "    if current_state:\n",
    "        print(f\"Currently in: \", current_state[-1])\n",
    "    message = event.get(\"messages\")\n",
    "    if message:\n",
    "        if isinstance(message, list):\n",
    "            message = message[-1]\n",
    "        if message.id not in _printed:\n",
    "            msg_repr = message.pretty_repr(html=True)\n",
    "            if len(msg_repr) > max_length:\n",
    "                msg_repr = msg_repr[:max_length] + \" ... (truncated)\"\n",
    "            print(msg_repr)\n",
    "            _printed.add(message.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa068b1a",
   "metadata": {},
   "source": [
    "## Part 1: Zero-shot Agent\n",
    "\n",
    "When building, it's best to start with the simplest working implementation and use an [evaluation tool like LangSmith](https://docs.smith.langchain.com/evaluation) to measure its efficacy. All else equal, prefer simple, scalable solutions to complicated ones. In this case, the single-graph approach has limitations. The bot may take undesired actions without user confirmation, struggle with complex queries, and lack focus in its responses. We'll address these issues later. \n",
    "\n",
    "In this section, we will define a simple Zero-shot agent as the assistant, give the agent **all** of our tools, and prompt it to use them judiciously to assist the user.\n",
    "\n",
    "The simple 2-node graph will look like the following:\n",
    "\n",
    "![Part 1 Diagram](../img/part-1-diagram.png)\n",
    "\n",
    "Start by defining the state.\n",
    "\n",
    "#### State\n",
    "\n",
    "Define our `StateGraph`'s state as a typed dictionary containing an append-only list of messages. These messages form the chat history, which is all the state our simple assistant needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3216948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897fbd63",
   "metadata": {},
   "source": [
    "#### Agent\n",
    "\n",
    "Next, define the assistant function. This function takes the graph state, formats it into a prompt, and then calls an LLM for it to predict the best response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd269bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastafoya/Documents/Projects/LangGraphAgent/langgraph-agent/venv/lib/python3.10/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The method `ChatAnthropic.bind_tools` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "\n",
    "\n",
    "class Assistant:\n",
    "    def __init__(self, runnable: Runnable):\n",
    "        self.runnable = runnable\n",
    "\n",
    "    def __call__(self, state: State, config: RunnableConfig):\n",
    "        while True:\n",
    "            passenger_id = config.get(\"passenger_id\", None)\n",
    "            state = {**state, \"user_info\": passenger_id}\n",
    "            result = self.runnable.invoke(state)\n",
    "            # If the LLM happens to return an empty response, we will re-prompt it\n",
    "            # for an actual response.\n",
    "            if not result.tool_calls and (\n",
    "                not result.content\n",
    "                or isinstance(result.content, list)\n",
    "                and not result.content[0].get(\"text\")\n",
    "            ):\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                break\n",
    "        return {\"messages\": result}\n",
    "\n",
    "\n",
    "# Haiku is faster and cheaper, but less accurate\n",
    "# llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=1)\n",
    "# You could swap LLMs, though you will likely want to update the prompts when\n",
    "# doing so!\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "\n",
    "primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful customer support assistant for Swiss Airlines. \"\n",
    "            \" Use the provided tools to search for flights, company policies, and other information to assist the user's queries. \"\n",
    "            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n",
    "            \" If a search comes up empty, expand your search before giving up.\"\n",
    "            \"\\n\\nCurrent user:\\n<User>\\n{user_info}\\n</User>\"\n",
    "            \"\\nCurrent time: {time}.\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ").partial(time=datetime.now())\n",
    "\n",
    "part_1_tools = [\n",
    "    TavilySearchResults(max_results=1),\n",
    "    fetch_user_flight_information,\n",
    "    search_flights,\n",
    "    lookup_policy,\n",
    "    update_ticket_to_new_flight,\n",
    "    cancel_ticket,\n",
    "    search_car_rentals,\n",
    "    book_car_rental,\n",
    "    update_car_rental,\n",
    "    cancel_car_rental,\n",
    "    search_hotels,\n",
    "    book_hotel,\n",
    "    update_hotel,\n",
    "    cancel_hotel,\n",
    "    search_trip_recommendations,\n",
    "    book_excursion,\n",
    "    update_excursion,\n",
    "    cancel_excursion,\n",
    "]\n",
    "part_1_assistant_runnable = primary_assistant_prompt | llm.bind_tools(part_1_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be1b8f1",
   "metadata": {},
   "source": [
    "#### Define Graph\n",
    "\n",
    "Now, create the graph. The graph is the final assistant for this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36064ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", Assistant(part_1_assistant_runnable))\n",
    "builder.add_node(\"tools\", create_tool_node_with_fallback(part_1_tools))\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.set_entry_point(\"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "# The checkpointer lets the graph persist its state\n",
    "# this is a complete memory for the entire graph.\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "part_1_graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a7e47a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(part_1_graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1042d045-76c5-45f5-ae12-8f29d3184338",
   "metadata": {},
   "source": [
    "#### Example Conversation\n",
    "\n",
    "Now it's time to try out our mighty chatbot! Let's run it over the following list of dialog turns. If it hits a \"RecursionLimit\", that means the agent wasn't able to get an answer in the allocated number of steps. That's OK! We have more tricks up our sleeve in later sections of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7443751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import uuid\n",
    "\n",
    "# Let's create an example conversation a user might have with the assistant\n",
    "tutorial_questions = [\n",
    "    \"Hi there, what time is my flight?\",\n",
    "    \"Am i allowed to update my flight to something sooner? I want to leave later today.\",\n",
    "    \"Update my flight to sometime next week then\",\n",
    "    \"The next available option is great\",\n",
    "    \"what about lodging and transportation?\",\n",
    "    \"Yeah i think i'd like an affordable hotel for my week-long stay (7 days). And I'll want to rent a car.\",\n",
    "    \"OK could you place a reservation for your recommended hotel? It sounds nice.\",\n",
    "    \"yes go ahead and book anything that's moderate expense and has availability.\",\n",
    "    \"Now for a car, what are my options?\",\n",
    "    \"Awesome let's just get the cheapest option. Go ahead and book for 7 days\",\n",
    "    \"Cool so now what recommendations do you have on excursions?\",\n",
    "    \"Are they available while I'm there?\",\n",
    "    \"interesting - i like the museums, what options are there? \",\n",
    "    \"OK great pick one and book it for my second day there.\",\n",
    "]\n",
    "breakpoint()\n",
    "# Update with the backup file so we can restart from the original place in each section\n",
    "shutil.copy(backup_file, db)\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # The passenger_id is used in our flight tools to\n",
    "        # fetch the user's flight information\n",
    "        \"passenger_id\": \"3442 587242\",\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "_printed = set()\n",
    "for question in tutorial_questions:\n",
    "    events = part_1_graph.stream(\n",
    "        {\"messages\": (\"user\", question)}, config, stream_mode=\"values\"\n",
    "    )\n",
    "    for event in events:\n",
    "        _print_event(event, _printed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aaae68-7791-4f5d-a98b-c0f3f9ed0eb0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Part 1 Review\n",
    "\n",
    "Our simple assistant is not bad! It was able to respond reasonably well for all the questions, quickly respond in-context, and successfully execute all our tasks. You can (check out an example LangSmith trace)[https://smith.langchain.com/public/f9e77b80-80ec-4837-98a8-254415cb49a1/r/26146720-d3f9-44b6-9bb9-9158cde61f9d] to get a better sense of how the LLM is prompted throughout the interactions above.\n",
    "\n",
    "If this were a simple Q&A bot, we'd probably be happy with the results above. Since our customer support bot is taking actions on behalf of the user, some of its behavior above is a bit concerning:\n",
    "\n",
    "1. The assistant booked a car when we were focusing on lodging, then had to cancel and rebook later on: oops! The user should have final say before booking to avoid unwanted feeds.\n",
    "2. The assistant struggled to search for recommendations. We could improve this by adding more verbose instructions and examples using the tool, but doing this for every tool can lead to a large prompt and overwhelmed agent.\n",
    "3. The assistant had to do an explicit search just to get the user's relevant information. We can save a lot of time by fetching the user's relevant travel details immediately so the assistant can directly respond.\n",
    "\n",
    "In the next section, we will address the first two of these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4bce47",
   "metadata": {},
   "source": [
    "## Part 2: Add Confirmation\n",
    "\n",
    "We will now add confirmation for the user before the agent performs any action.\n",
    "We will use `interrupt_before` to pause the graph to return control to the user before executing any of the tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4874ccfc",
   "metadata": {},
   "source": [
    "### State & Assistant\n",
    "\n",
    "Our graph state and LLM calling is nearly identical to Part 1 except:\n",
    "\n",
    "We've added a user_info field that will be eagerly populated by our graph\n",
    "We can use the state directly in the Assistant object rather than using the configurable params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91a19ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    user_info: str\n",
    "\n",
    "\n",
    "class Assistant:\n",
    "    def __init__(self, runnable: Runnable):\n",
    "        self.runnable = runnable\n",
    "\n",
    "    def __call__(self, state: State, config: RunnableConfig):\n",
    "        while True:\n",
    "            passenger_id = config.get(\"passenger_id\", None)\n",
    "            result = self.runnable.invoke(state)\n",
    "            # If the LLM happens to return an empty response, we will re-prompt it\n",
    "            # for an actual response.\n",
    "            if not result.tool_calls and (\n",
    "                not result.content\n",
    "                or isinstance(result.content, list)\n",
    "                and not result.content[0].get(\"text\")\n",
    "            ):\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                break\n",
    "        return {\"messages\": result}\n",
    "\n",
    "\n",
    "# Haiku is faster and cheaper, but less accurate\n",
    "# llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=1)\n",
    "# You could also use OpenAI or another model, though you will likely have\n",
    "# to adapt the prompts\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "\n",
    "assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful customer support assistant for Swiss Airlines. \"\n",
    "            \" Use the provided tools to search for flights, company policies, and other information to assist the user's queries. \"\n",
    "            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n",
    "            \" If a search comes up empty, expand your search before giving up.\"\n",
    "            \"\\n\\nCurrent user:\\n\\n{user_info}\\n\"\n",
    "            \"\\nCurrent time: {time}.\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ").partial(time=datetime.now())\n",
    "\n",
    "part_2_tools = [\n",
    "    TavilySearchResults(max_results=1),\n",
    "    fetch_user_flight_information,\n",
    "    search_flights,\n",
    "    lookup_policy,\n",
    "    update_ticket_to_new_flight,\n",
    "    cancel_ticket,\n",
    "    search_car_rentals,\n",
    "    book_car_rental,\n",
    "    update_car_rental,\n",
    "    cancel_car_rental,\n",
    "    search_hotels,\n",
    "    book_hotel,\n",
    "    update_hotel,\n",
    "    cancel_hotel,\n",
    "    search_trip_recommendations,\n",
    "    book_excursion,\n",
    "    update_excursion,\n",
    "    cancel_excursion,\n",
    "]\n",
    "part_2_assistant_runnable = assistant_prompt | llm.bind_tools(part_2_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae30eb8",
   "metadata": {},
   "source": [
    "### Define the Graph\n",
    "We will now make 2 changes to address the concerns of the part 1.\n",
    "1. Add an interrupt before using a tool.\n",
    "2. Explicitly populate the user state within the first node so the assitant doesn't have to use a tool just to learn about the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbb9f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "def user_info(state: State):\n",
    "    return {\"user_info\": fetch_user_flight_information.invoke({})}\n",
    "\n",
    "# NEW: The fetch_user_info node runds first, meaning our assistant can see the user's flight information without\n",
    "# having to take an action\n",
    "builder.add_node(\"fetch_user_info\", user_info)\n",
    "builder.set_entry_point(\"fetch_user_info\")\n",
    "builder.add_node(\"assistant\", Assistant(part_2_assistant_runnable))\n",
    "builder.add_node(\"action\", create_tool_node_with_fallback(part_2_tools))\n",
    "builder.add_edge(\"fetch_user_info\", \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\", tools_condition, {\"action\": \"action\", END:END}\n",
    ")\n",
    "builder.add_edge(\"action\", \"assistant\")\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "part_2_graph = builder.compile(\n",
    "    checkpointer=memory,\n",
    "    # NEW: The graph will always halt before executing the \"action\" node.\n",
    "    # THe user can approve or reject (or even alter the request) before\n",
    "    # the assistant continues\n",
    "    interrupt_before=[\"action\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e91debee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(part_2_graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec0a7a",
   "metadata": {},
   "source": [
    "### Example Conversation\n",
    "Now lets try the new chatbot using a list of dialog turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35913b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi there, what time is my flight?\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'Number of request tokens has exceeded your daily rate limit (https://docs.anthropic.com/en/api/rate-limits); see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later. You may also contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m tutorial_questions:\n\u001b[1;32m     22\u001b[0m     events \u001b[38;5;241m=\u001b[39m part_2_graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m     23\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, question)}, config, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m     )\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m events:\n\u001b[1;32m     26\u001b[0m         _print_event(event, _printed)\n\u001b[1;32m     27\u001b[0m     snapshot \u001b[38;5;241m=\u001b[39m part_2_graph\u001b[38;5;241m.\u001b[39mget_state(config)\n",
      "File \u001b[0;32m~/Documents/Projects/LangGraphAgent/langgraph-agent/venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:839\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m    832\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m    833\u001b[0m     futures,\n\u001b[1;32m    834\u001b[0m     return_when\u001b[38;5;241m=\u001b[39mconcurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFIRST_EXCEPTION,\n\u001b[1;32m    835\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 839\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;66;03m# combine pending writes from all tasks\u001b[39;00m\n\u001b[1;32m    842\u001b[0m pending_writes \u001b[38;5;241m=\u001b[39m deque[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]()\n",
      "File \u001b[0;32m~/Documents/Projects/LangGraphAgent/langgraph-agent/venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1342\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m   1340\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1341\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1342\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m   1347\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/Documents/Projects/LangGraphAgent/langgraph-agent/venv/lib/python3.10/site-packages/langgraph/pregel/retry.py:66\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     64\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/LangGraphAgent/langgraph-agent/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:2368\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2366\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2367\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2368\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2369\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2370\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2371\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2372\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2373\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2374\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2375\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2376\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/Projects/LangGraphAgent/langgraph-agent/venv/lib/python3.10/site-packages/langgraph/utils.py:89\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     83\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, config)\n\u001b[1;32m     84\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     85\u001b[0m         {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m: config}\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs\n\u001b[1;32m     88\u001b[0m     )\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m, in \u001b[0;36mAssistant.__call__\u001b[0;34m(self, state, config)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     passenger_id \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassenger_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 24\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunnable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# If the LLM happens to return an empty response, we will re-prompt it\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# for an actual response.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mtool_calls \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m     ):\n",
      "File \u001b[0;32m~/Documents/Projects/LangGraphAgent/langgraph-agent/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:2368\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2366\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2367\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2368\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2369\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2370\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2371\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2372\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2373\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2374\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2375\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2376\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/Projects/LangGraphAgent/langgraph-agent/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:4396\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4391\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4392\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4393\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4394\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4395\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4397\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4398\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4399\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4400\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/LangGraphAgent/langgraph-agent/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:170\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    166\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    167\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    169\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 170\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    180\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Documents/Projects/LangGraphAgent/langgraph-agent/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:599\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    593\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    598\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/LangGraphAgent/langgraph-agent/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:456\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    455\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    457\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    458\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    460\u001b[0m ]\n\u001b[1;32m    461\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/Documents/Projects/LangGraphAgent/langgraph-agent/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:446\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 446\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m         )\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Documents/Projects/LangGraphAgent/langgraph-agent/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:671\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 671\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/Projects/LangGraphAgent/langgraph-agent/venv/lib/python3.10/site-packages/langchain_anthropic/chat_models.py:512\u001b[0m, in \u001b[0;36mChatAnthropic._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generate_from_stream(stream_iter)\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _tools_in_params(params):\n\u001b[0;32m--> 512\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    514\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[0;32m~/Documents/Projects/LangGraphAgent/langgraph-agent/venv/lib/python3.10/site-packages/anthropic/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/LangGraphAgent/langgraph-agent/venv/lib/python3.10/site-packages/anthropic/resources/beta/tools/messages.py:848\u001b[0m, in \u001b[0;36mMessages.create\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    845\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m600\u001b[39m,\n\u001b[1;32m    846\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ToolsBetaMessage \u001b[38;5;241m|\u001b[39m Stream[MessageStreamEvent]:\n\u001b[1;32m    847\u001b[0m     extra_headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic-beta\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools-2024-04-04\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[0;32m--> 848\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/messages?beta=tools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop_sequences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessageCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mToolsBetaMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/LangGraphAgent/langgraph-agent/venv/lib/python3.10/site-packages/anthropic/_base_client.py:1239\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1226\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1227\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1235\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1236\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1237\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1238\u001b[0m     )\n\u001b[0;32m-> 1239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/Projects/LangGraphAgent/langgraph-agent/venv/lib/python3.10/site-packages/anthropic/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/LangGraphAgent/langgraph-agent/venv/lib/python3.10/site-packages/anthropic/_base_client.py:1004\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1003\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1004\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Documents/Projects/LangGraphAgent/langgraph-agent/venv/lib/python3.10/site-packages/anthropic/_base_client.py:1052\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1052\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/LangGraphAgent/langgraph-agent/venv/lib/python3.10/site-packages/anthropic/_base_client.py:1004\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1003\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1004\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Documents/Projects/LangGraphAgent/langgraph-agent/venv/lib/python3.10/site-packages/anthropic/_base_client.py:1052\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1052\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/LangGraphAgent/langgraph-agent/venv/lib/python3.10/site-packages/anthropic/_base_client.py:1019\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1016\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1018\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1019\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1022\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1023\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1026\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1027\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'Number of request tokens has exceeded your daily rate limit (https://docs.anthropic.com/en/api/rate-limits); see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later. You may also contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import uuid\n",
    "\n",
    "# Update with the backup file so we can restart from the original place in each section\n",
    "shutil.copy(backup_file, db)\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # The passenger_id is used in our flight tools to\n",
    "        # fetch the user's flight information\n",
    "        \"passenger_id\": \"3442 587242\",\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "_printed = set()\n",
    "\n",
    "# Reause the same questions from the first part\n",
    "for question in tutorial_questions:\n",
    "    events = part_2_graph.stream(\n",
    "        {\"messages\": (\"user\", question)}, config, stream_mode=\"values\"\n",
    "    )\n",
    "    for event in events:\n",
    "        _print_event(event, _printed)\n",
    "    snapshot = part_2_graph.get_state(config)\n",
    "    while snapshot.next:\n",
    "        # We have an interrupt here, because the agent is trying to use a tool.\n",
    "        user_input = input(\n",
    "            \"Do you approve the execution of the above actions? Type 'y' to\"\n",
    "            \" continue. Otherwise, explain your requested changes.\\n\\n.\"\n",
    "        )\n",
    "        if user_input.strip() == \"y\":\n",
    "            result = part_2_graph.invoke(\n",
    "                None,\n",
    "                config\n",
    "            )\n",
    "        else:\n",
    "            # Satisfy the tool invocation by providing instructions on the\n",
    "            # requested changes\n",
    "            result = part_2_graph.invoke(\n",
    "                {\n",
    "                    \"messages\": [\n",
    "                        ToolMessage(\n",
    "                            tool_call_id=event[\"messages\"][-1].tool_calls[0][\"id\"],\n",
    "                            content=f\"API call denied by user. Reasoning: '{user_input}'. Continue assisting, accounting for the user's input.\",\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                config\n",
    "            )\n",
    "        snapshot = part_2_graph.get_state(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8f508f",
   "metadata": {},
   "source": [
    "#### Part 2 Review\n",
    "\n",
    "Now our assistant was able to save a step to respond with our flight details. We also completely controlled which actions were performed. This all worked using LangGraph's interrupts and checkpointers. The interrupt pauses graph execution, its state safely persisted using your configured checkpointer. The user can then start it up at any time by running it with the right config.\n",
    "\n",
    "See an example LangSmith trace to get a better sense of how the graph is running. Note from this trace that you typically resume a flow by invoking the graph with (None, config). The state is loaded from the checkpoint as if it never was interrupted.\n",
    "\n",
    "This graph worked pretty well! We didn't really need to be involved in EVERY assistant action, though...\n",
    "\n",
    "In the next section, we will reorganize our graph so that we can interrupt only on the \"sensitive\" actions that actually write to the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606ba5e9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3f96b36",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
